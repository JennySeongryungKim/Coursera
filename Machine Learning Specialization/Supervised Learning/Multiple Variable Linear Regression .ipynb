{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54af3568-5990-4413-adca-56dcdfd6365a",
   "metadata": {},
   "source": [
    "## Multiple Variable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce84ca1-de85-4e72-92d6-5930e3b4b352",
   "metadata": {},
   "source": [
    "### 1.1 Goals\n",
    "- Extend our regression model  routines to support multiple features\n",
    "    - Extend data structures to support multiple features\n",
    "    - Rewrite prediction, cost and gradient routines to support multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd24bd2-d53f-4a9e-8144-d7b147fafaa4",
   "metadata": {},
   "source": [
    "### 1.2 Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "023fd03b-5708-47f1-86bb-2f8fbfb2c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6627e0-528e-40ca-a152-c3c9cb2e2582",
   "metadata": {},
   "source": [
    "### 2. Problem Statement\n",
    "- admission prediction\n",
    "- X variables: GRE Score, TOEFL Score, University Rating, SOP, LOR, CGPA, Research\n",
    "- Y variable: Chance of Admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa4ff4e-ef16-4a09-943f-ceb80d7795dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admission_df = pd.read_csv(\"~/Downloads/prediction of Graduate Admissions/Admission_Predict_Ver1.1.csv\")\n",
    "admission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5499681e-24e0-45e6-b5d3-ea8f0ae9f1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train =      GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
      "0          337          118                  4  4.5   4.5  9.65         1\n",
      "1          324          107                  4  4.0   4.5  8.87         1\n",
      "2          316          104                  3  3.0   3.5  8.00         1\n",
      "3          322          110                  3  3.5   2.5  8.67         1\n",
      "4          314          103                  2  2.0   3.0  8.21         0\n",
      "..         ...          ...                ...  ...   ...   ...       ...\n",
      "495        332          108                  5  4.5   4.0  9.02         1\n",
      "496        337          117                  5  5.0   5.0  9.87         1\n",
      "497        330          120                  5  4.5   5.0  9.56         1\n",
      "498        312          103                  4  4.0   5.0  8.43         0\n",
      "499        327          113                  4  4.5   4.5  9.04         0\n",
      "\n",
      "[500 rows x 7 columns]\n",
      "y_train = 0      0.92\n",
      "1      0.76\n",
      "2      0.72\n",
      "3      0.80\n",
      "4      0.65\n",
      "       ... \n",
      "495    0.87\n",
      "496    0.96\n",
      "497    0.93\n",
      "498    0.73\n",
      "499    0.84\n",
      "Name: Chance of Admit , Length: 500, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#x_train is the input variable\n",
    "#y_train is the target\n",
    "admission_df.columns\n",
    "x_train = admission_df.drop(columns=['Chance of Admit ','Serial No.'])\n",
    "y_train = admission_df['Chance of Admit ']\n",
    "print(f\"x_train = {x_train}\")\n",
    "print(f\"y_train = {y_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3436a4-bdc4-4647-a8f7-057831ec7128",
   "metadata": {},
   "source": [
    "### Matrix X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96397356-7f95-444c-bc5b-730d37043c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (500, 7), X Type:<class 'pandas.core.frame.DataFrame'>\n",
      "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
      "0          337          118                  4  4.5   4.5  9.65         1\n",
      "1          324          107                  4  4.0   4.5  8.87         1\n",
      "2          316          104                  3  3.0   3.5  8.00         1\n",
      "3          322          110                  3  3.5   2.5  8.67         1\n",
      "4          314          103                  2  2.0   3.0  8.21         0\n",
      "..         ...          ...                ...  ...   ...   ...       ...\n",
      "495        332          108                  5  4.5   4.0  9.02         1\n",
      "496        337          117                  5  5.0   5.0  9.87         1\n",
      "497        330          120                  5  4.5   5.0  9.56         1\n",
      "498        312          103                  4  4.0   5.0  8.43         0\n",
      "499        327          113                  4  4.5   4.5  9.04         0\n",
      "\n",
      "[500 rows x 7 columns]\n",
      "y Shape: (500,), y Type:<class 'pandas.core.series.Series'>)\n",
      "0      0.92\n",
      "1      0.76\n",
      "2      0.72\n",
      "3      0.80\n",
      "4      0.65\n",
      "       ... \n",
      "495    0.87\n",
      "496    0.96\n",
      "497    0.93\n",
      "498    0.73\n",
      "499    0.84\n",
      "Name: Chance of Admit , Length: 500, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape: {x_train.shape}, X Type:{type(x_train)}\")\n",
    "print(x_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e304d7-42aa-47b2-b6bf-0866eadbc61a",
   "metadata": {},
   "source": [
    "### 2.2 Parameter vector w,b\n",
    "\n",
    "- $w$ is a vector with $n$ elements\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "* $b$ is a scalar parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe594418-ec13-4ef4-ad3a-c80bf3830de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_init = 0\n",
    "w_init = np.random.rand(x_train.shape[1]) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5abba-f2eb-49e2-ab85-041d16d68037",
   "metadata": {},
   "source": [
    "### 3.Model Prediction With Multiple Variables\n",
    "\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d0ebb-7ed9-482b-b834-d96ffb94d108",
   "metadata": {},
   "source": [
    "### 3.1 single Prediction, vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e162b704-2fb6-48d0-8cea-058930cd3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"\n",
    "    Single prediction using linear regression\n",
    "\n",
    "    Args:\n",
    "        x (pd.Series): Shape (n,) example with multiple features\n",
    "        w (pd.Series or np.ndarray): Shape (n,) model parameters (weights)\n",
    "        b (scalar): model parameter (bias)\n",
    "\n",
    "    Returns:\n",
    "        p (scalar): prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dot product of x and w (element-wise multiplication and summing)\n",
    "    p = np.dot(x, w) + b\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "690d316f-ccf6-4a79-91d5-862a42dd9be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for all samples: [4.23175909 4.00517072 3.88374543 3.99471127 3.83603948 4.13736431\n",
      " 3.97893689 3.77841933 3.70599779 3.98428251 3.99114853 4.07019341\n",
      " 4.08876281 3.85833682 3.83618909 3.87239316 3.9263443  3.93664325\n",
      " 3.9639787  3.74878606 3.86839251 4.0538104  4.14511939 4.22469616\n",
      " 4.22734736 4.28263438 4.00802487 3.64019254 3.55954791 3.75073147\n",
      " 3.66673001 3.98515812 4.22559313 4.2177922  4.12426409 4.01627039\n",
      " 3.75638111 3.70941465 3.76160741 3.84308852 3.87185997 3.87800963\n",
      " 3.8680042  4.17133695 4.08819997 4.02850111 4.12487056 4.25868045\n",
      " 3.99967933 4.05685876 3.80074953 3.78995011 4.17074657 4.04092903\n",
      " 3.99148056 3.90382184 3.84843396 3.6688023  3.67352108 3.81350306\n",
      " 3.77259362 3.77942148 3.77875289 3.9060339  4.02863674 4.0485178\n",
      " 4.07273276 3.91293327 3.958437   4.11962609 4.19999857 4.17859211\n",
      " 4.03503215 3.92802345 3.891423   4.08334591 4.05619486 3.68648738\n",
      " 3.60585386 3.54592643 3.84510375 4.28500489 4.01356686 4.0693902\n",
      " 4.23477952 3.91877593 3.90628258 3.91573925 3.91607984 3.95228502\n",
      " 3.92794192 3.68189488 3.6652796  3.67535654 3.70162813 3.72053054\n",
      " 3.7457394  4.18369165 4.2011405  4.04559228 3.97130282 3.84208975\n",
      " 3.88708948 3.90267978 4.05262741 3.95723509 4.09315957 4.22105558\n",
      " 4.17169148 3.79791598 3.82915157 3.9945369  3.78406706 3.97906899\n",
      " 3.85150055 3.87605299 3.72095249 3.63836998 3.64892613 3.99311478\n",
      " 4.21708946 4.21949602 3.83184436 3.85289446 3.76437268 3.69135808\n",
      " 4.04069819 3.9745364  4.05269153 4.20853533 4.2118179  3.81081432\n",
      " 3.84393849 4.04604573 4.1454887  3.92737368 3.85864476 3.82318901\n",
      " 4.09548347 3.9423709  4.06018622 4.17108869 4.14421239 4.27516715\n",
      " 4.03626608 3.98555053 3.86677864 4.06685135 4.22032742 3.85564097\n",
      " 4.1589738  4.1804719  4.04406451 3.96532638 4.00919157 3.89101716\n",
      " 3.8665991  3.79836362 3.79054875 3.64007242 3.82897451 3.64434267\n",
      " 3.94422546 3.90218175 4.09021238 4.02050456 3.7505761  3.82282369\n",
      " 3.599607   3.77580655 3.80949816 4.19498722 4.0190135  4.05391873\n",
      " 4.0139738  4.00712572 4.16939392 3.97100631 3.84910969 3.77920774\n",
      " 3.74068905 3.79644967 3.68595406 3.93380241 3.89297173 4.09431748\n",
      " 3.92431247 4.21336253 4.1490278  4.07081614 4.05100987 4.03287822\n",
      " 4.06150791 4.23057414 3.93165957 3.82768343 3.79100615 3.84118107\n",
      " 3.85705412 3.89888436 3.87616692 3.92966209 4.28303193 4.22733984\n",
      " 3.73834991 3.63284478 3.82116178 3.8189013  3.79423092 3.7579166\n",
      " 4.02604437 4.07685595 4.26739276 4.2158901  4.17120885 4.160028\n",
      " 4.04043404 3.99511399 4.01860209 3.85065559 3.85810213 3.94639394\n",
      " 4.06154012 3.85538466 3.77932984 3.63951159 3.85201785 3.90011405\n",
      " 3.98560616 4.02545671 3.87023557 3.92941927 3.86407629 3.72582302\n",
      " 4.1286372  4.06891696 4.0621266  4.1296475  3.81802013 3.65787691\n",
      " 3.65570849 3.86214817 4.06303568 4.06277092 3.88499469 4.05867851\n",
      " 3.89130227 3.82683944 4.02097261 4.00372783 3.9146527  3.82165793\n",
      " 3.85405884 4.18538275 4.04531018 3.87876794 3.78409383 3.93572902\n",
      " 3.98774966 4.19236008 4.05128871 3.85296156 3.79606807 4.00692062\n",
      " 4.01148624 3.82553831 3.83486576 3.89474362 4.0941733  3.87622032\n",
      " 3.79167786 3.62485687 3.56262133 3.75957554 3.80813961 3.99781235\n",
      " 4.12263726 3.87587025 3.79551179 3.75338161 3.83844108 3.96656695\n",
      " 3.87802492 3.98852323 4.20622091 4.15624323 4.22467976 4.08715196\n",
      " 3.89733234 3.91288856 3.7969437  3.69097335 3.67540341 3.78303861\n",
      " 3.83730435 3.84173647 3.86175637 4.0857061  4.12559426 3.85993191\n",
      " 3.8220132  3.93386862 3.93942939 3.9803269  3.8554701  3.98189518\n",
      " 4.01454773 4.05917941 3.88619747 3.87486602 3.92032907 4.05292424\n",
      " 3.89444285 3.71131382 3.79011831 3.79685848 3.6636107  3.65521814\n",
      " 4.00966262 4.07575532 3.92219632 3.95923551 3.88462278 3.74356139\n",
      " 3.86688979 4.09633118 3.67325848 3.64672073 4.04724662 3.61418485\n",
      " 4.07056842 3.83555257 3.83127011 3.94929392 3.90017349 4.05251716\n",
      " 3.96030934 4.19978056 4.00804709 4.00240265 3.87456903 4.03591968\n",
      " 3.82620792 3.76340613 3.58855536 3.78966923 3.67865054 3.58960022\n",
      " 3.67916765 3.81791424 3.93050651 4.03477321 3.72473137 3.7189077\n",
      " 3.64115525 3.89397672 4.03803785 3.74974404 3.85383315 3.92863753\n",
      " 4.01323669 4.17545069 4.21940924 3.76988057 3.84675582 4.12246538\n",
      " 3.93019246 3.74297557 3.57761344 3.67009697 3.80263214 4.0159631\n",
      " 4.22961528 3.97391457 3.8605658  3.72445336 3.61632411 3.57727577\n",
      " 3.68178596 3.77414705 3.94592182 3.92086314 4.03631612 3.70049847\n",
      " 4.21858518 4.21742289 3.71739713 3.79533197 3.60758644 3.96572152\n",
      " 3.82381628 3.90598431 4.06551279 3.88283931 4.09049452 4.01846144\n",
      " 3.99394437 4.15499499 3.84431394 4.18879361 3.73312222 3.87401587\n",
      " 4.00607152 4.13941849 3.79484859 3.70009734 3.92738061 3.6830066\n",
      " 3.67835335 3.65553448 3.66281176 3.73796621 3.83875211 3.85579212\n",
      " 3.99872819 4.02221031 3.87643084 3.8676212  3.8779017  3.77840107\n",
      " 3.67794738 4.01122539 4.01937354 4.22230215 4.0894411  4.04353926\n",
      " 3.87318248 3.81563509 3.86272402 4.23919737 3.84920486 3.99640995\n",
      " 4.05547089 3.97142558 3.78467264 3.82025785 3.86417931 3.88621938\n",
      " 3.94423541 3.83268976 3.76158589 4.07561036 4.15897282 4.05561056\n",
      " 4.07073064 4.13966277 4.15034532 3.97185956 3.88683228 3.85515295\n",
      " 4.00734973 4.06777104 4.13191168 3.89972473 3.83177854 3.73795087\n",
      " 3.66793052 3.61448823 3.79763446 4.10207905 3.94129311 3.71811171\n",
      " 3.81253046 3.80453646 3.63502349 3.71751855 3.83472657 3.89736793\n",
      " 4.02760156 4.08462908 4.00106579 3.81976638 4.11847147 3.86664856\n",
      " 3.8216346  3.7090033  3.76097803 3.83011521 3.90795301 4.04498112\n",
      " 3.91816617 3.97500919 4.0869554  3.79153428 3.91376685 3.80097423\n",
      " 3.87859272 4.09794393 4.01698167 3.82924293 3.8086458  3.66881132\n",
      " 3.70008327 3.63963137 3.68992458 4.09350212 4.23530434 4.19622565\n",
      " 3.85938987 4.09131588]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction\n",
    "f_wb = predict(x_train, w_init, b_init)\n",
    "\n",
    "print(\"Predictions for all samples:\", f_wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948f508-1173-4c60-92eb-e4a31f1c3534",
   "metadata": {},
   "source": [
    "### 4.Compute Cost With Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "195ef78b-726a-4d3d-8807-d19d83599463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Compute cost using pandas for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      X (pd.DataFrame): Data, m examples with n features\n",
    "      y (pd.Series)   : Target values\n",
    "      w (pd.Series)   : Model parameters (weights)\n",
    "      b (scalar)      : Model parameter (bias)\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): The computed cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Compute predictions: f_wb = X.dot(w) + b\n",
    "    predictions = X.dot(w) + b  # Shape: (m,)\n",
    "    \n",
    "    # Compute the cost: J(w, b) = (1/2m) * sum((f_wb_i - y_i)^2)\n",
    "    cost = ((predictions - y) ** 2).sum() / (2 * m)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a4ef89a2-292f-4dfe-b6a6-8ca9ffe38854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 5.124632281692698\n"
     ]
    }
   ],
   "source": [
    "cost = compute_cost(x_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a3648-cf0e-4d83-816a-5afa3be8cdc3",
   "metadata": {},
   "source": [
    "### 5. Gradient Descent With Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75673309-5dd9-4034-9447-b150be799dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression using pandas.\n",
    "\n",
    "    Args:\n",
    "      X (pd.DataFrame): Data, m examples with n features\n",
    "      y (pd.Series): target values\n",
    "      w (np.ndarray): model parameters (weights)\n",
    "      b (scalar): model parameter (bias)\n",
    "\n",
    "    Returns:\n",
    "      dj_dw (np.ndarray): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape  # number of examples (m) and number of features (n)\n",
    "    dj_dw = np.zeros(n)  # Initialize the gradient for weights\n",
    "    dj_db = 0  # Initialize the gradient for bias\n",
    "\n",
    "    # Loop through all examples\n",
    "    for i in range(m):\n",
    "        f_wb = np.dot(X.iloc[i], w) + b  # Model prediction\n",
    "        error = f_wb - y.iloc[i]         # Error: prediction - actual value\n",
    "        \n",
    "        # Update gradients\n",
    "        dj_db += error\n",
    "        dj_dw += error * X.iloc[i].values  # element-wise product of error and feature\n",
    "        \n",
    "    dj_dw /= m  # Average gradient for weights\n",
    "    dj_db /= m  # Average gradient for bias\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef822654-1b55-4810-acde-1bb1444289c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn w and b.\n",
    "\n",
    "    Args:\n",
    "      X (pd.DataFrame or np.ndarray): Data, m examples with n features\n",
    "      y (pd.Series or np.ndarray): target values\n",
    "      w_in (np.ndarray): initial model parameters  \n",
    "      b_in (scalar): initial model parameter\n",
    "      cost_function: function to compute cost\n",
    "      gradient_function: function to compute the gradient\n",
    "      alpha (float): Learning rate\n",
    "      num_iters (int): number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "      w (np.ndarray): Updated weights \n",
    "      b (scalar): Updated bias\n",
    "      J_history (list): Cost history over iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = []  # List to store cost at each iteration\n",
    "    w = np.copy(w_in)  # Avoid modifying original w\n",
    "    b = b_in\n",
    "    \n",
    "    # Gradient descent loop\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Compute gradients\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)\n",
    "\n",
    "        # Gradient clipping (optional)\n",
    "        max_gradient = 1e4  # Threshold to clip the gradients (aggressive clipping)\n",
    "        dj_dw = np.clip(dj_dw, -max_gradient, max_gradient)\n",
    "        dj_db = np.clip(dj_db, -max_gradient, max_gradient)\n",
    "\n",
    "        # Update parameters (weights and bias)\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "\n",
    "        # Compute and store the cost at each iteration\n",
    "        J_history.append(cost_function(X, y, w, b))\n",
    "\n",
    "        # Print the cost every 10 iterations\n",
    "        if i % max(1, num_iters // 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}\")\n",
    "\n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4d6fa7a-adf5-4363-90a5-9c30334b7c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     0.08\n",
      "Iteration  100: Cost     0.01\n",
      "Iteration  200: Cost     0.01\n",
      "Iteration  300: Cost     0.00\n",
      "Iteration  400: Cost     0.00\n",
      "Iteration  500: Cost     0.00\n",
      "Iteration  600: Cost     0.00\n",
      "Iteration  700: Cost     0.00\n",
      "Iteration  800: Cost     0.00\n",
      "Iteration  900: Cost     0.00\n",
      "Final weights: [-0.00044135  0.00727056  0.00560463  0.01017528  0.00576603  0.00128469\n",
      "  0.00312425]\n",
      "Final bias: -5.411274550921894e-05\n"
     ]
    }
   ],
   "source": [
    "#learning rate, iteration\n",
    "alpha =  1e-5\n",
    "num_iters = 1000\n",
    "\n",
    "w_final, b_final, cost_history = gradient_descent(x_train, y_train, w_init, b_init, compute_cost, compute_gradient, alpha, num_iters)\n",
    "\n",
    "print(\"Final weights:\", w_final)\n",
    "print(\"Final bias:\", b_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
