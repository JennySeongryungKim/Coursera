{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54af3568-5990-4413-adca-56dcdfd6365a",
   "metadata": {},
   "source": [
    "## Multiple Variable Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce84ca1-de85-4e72-92d6-5930e3b4b352",
   "metadata": {},
   "source": [
    "### 1.1 Goals\n",
    "- Extend our regression model  routines to support multiple features\n",
    "    - Extend data structures to support multiple features\n",
    "    - Rewrite prediction, cost and gradient routines to support multiple features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd24bd2-d53f-4a9e-8144-d7b147fafaa4",
   "metadata": {},
   "source": [
    "### 1.2 Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "023fd03b-5708-47f1-86bb-2f8fbfb2c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6627e0-528e-40ca-a152-c3c9cb2e2582",
   "metadata": {},
   "source": [
    "### 2. Problem Statement\n",
    "- admission prediction\n",
    "- X variables: GRE Score, TOEFL Score, University Rating, SOP, LOR, CGPA, Research\n",
    "- Y variable: Chance of Admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afa4ff4e-ef16-4a09-943f-ceb80d7795dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admission_df = pd.read_csv(\"~/Downloads/prediction of Graduate Admissions/Admission_Predict_Ver1.1.csv\")\n",
    "admission_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5499681e-24e0-45e6-b5d3-ea8f0ae9f1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train =      Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
      "0             1        337          118                  4  4.5   4.5  9.65   \n",
      "1             2        324          107                  4  4.0   4.5  8.87   \n",
      "2             3        316          104                  3  3.0   3.5  8.00   \n",
      "3             4        322          110                  3  3.5   2.5  8.67   \n",
      "4             5        314          103                  2  2.0   3.0  8.21   \n",
      "..          ...        ...          ...                ...  ...   ...   ...   \n",
      "495         496        332          108                  5  4.5   4.0  9.02   \n",
      "496         497        337          117                  5  5.0   5.0  9.87   \n",
      "497         498        330          120                  5  4.5   5.0  9.56   \n",
      "498         499        312          103                  4  4.0   5.0  8.43   \n",
      "499         500        327          113                  4  4.5   4.5  9.04   \n",
      "\n",
      "     Research  \n",
      "0           1  \n",
      "1           1  \n",
      "2           1  \n",
      "3           1  \n",
      "4           0  \n",
      "..        ...  \n",
      "495         1  \n",
      "496         1  \n",
      "497         1  \n",
      "498         0  \n",
      "499         0  \n",
      "\n",
      "[500 rows x 8 columns]\n",
      "y_train = 0      0.92\n",
      "1      0.76\n",
      "2      0.72\n",
      "3      0.80\n",
      "4      0.65\n",
      "       ... \n",
      "495    0.87\n",
      "496    0.96\n",
      "497    0.93\n",
      "498    0.73\n",
      "499    0.84\n",
      "Name: Chance of Admit , Length: 500, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#x_train is the input variable\n",
    "#y_train is the target\n",
    "admission_df.columns\n",
    "x_train = admission_df.drop(columns=['Chance of Admit '])\n",
    "y_train = admission_df['Chance of Admit ']\n",
    "print(f\"x_train = {x_train}\")\n",
    "print(f\"y_train = {y_train}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3436a4-bdc4-4647-a8f7-057831ec7128",
   "metadata": {},
   "source": [
    "### Matrix X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "96397356-7f95-444c-bc5b-730d37043c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (500, 8), X Type:<class 'pandas.core.frame.DataFrame'>\n",
      "     Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
      "0             1        337          118                  4  4.5   4.5  9.65   \n",
      "1             2        324          107                  4  4.0   4.5  8.87   \n",
      "2             3        316          104                  3  3.0   3.5  8.00   \n",
      "3             4        322          110                  3  3.5   2.5  8.67   \n",
      "4             5        314          103                  2  2.0   3.0  8.21   \n",
      "..          ...        ...          ...                ...  ...   ...   ...   \n",
      "495         496        332          108                  5  4.5   4.0  9.02   \n",
      "496         497        337          117                  5  5.0   5.0  9.87   \n",
      "497         498        330          120                  5  4.5   5.0  9.56   \n",
      "498         499        312          103                  4  4.0   5.0  8.43   \n",
      "499         500        327          113                  4  4.5   4.5  9.04   \n",
      "\n",
      "     Research  \n",
      "0           1  \n",
      "1           1  \n",
      "2           1  \n",
      "3           1  \n",
      "4           0  \n",
      "..        ...  \n",
      "495         1  \n",
      "496         1  \n",
      "497         1  \n",
      "498         0  \n",
      "499         0  \n",
      "\n",
      "[500 rows x 8 columns]\n",
      "y Shape: (500,), y Type:<class 'pandas.core.series.Series'>)\n",
      "0      0.92\n",
      "1      0.76\n",
      "2      0.72\n",
      "3      0.80\n",
      "4      0.65\n",
      "       ... \n",
      "495    0.87\n",
      "496    0.96\n",
      "497    0.93\n",
      "498    0.73\n",
      "499    0.84\n",
      "Name: Chance of Admit , Length: 500, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(f\"X shape: {x_train.shape}, X Type:{type(x_train)}\")\n",
    "print(x_train)\n",
    "print(f\"y Shape: {y_train.shape}, y Type:{type(y_train)})\")\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e304d7-42aa-47b2-b6bf-0866eadbc61a",
   "metadata": {},
   "source": [
    "### 2.2 Parameter vector w,b\n",
    "\n",
    "- $w$ is a vector with $n$ elements\n",
    "  - Each element contains the parameter associated with one feature.\n",
    "\n",
    "$$\\mathbf{w} = \\begin{pmatrix}\n",
    "w_0 \\\\ \n",
    "w_1 \\\\\n",
    "\\cdots\\\\\n",
    "w_{n-1}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "* $b$ is a scalar parameter.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "fe594418-ec13-4ef4-ad3a-c80bf3830de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "b_init = 0\n",
    "w_init = np.random.rand(x_train.shape[1]) * 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d5abba-f2eb-49e2-ab85-041d16d68037",
   "metadata": {},
   "source": [
    "### 3.Model Prediction With Multiple Variables\n",
    "\n",
    "The model's prediction with multiple variables is given by the linear model:\n",
    "\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) =  w_0x_0 + w_1x_1 +... + w_{n-1}x_{n-1} + b \\tag{1}$$\n",
    "or in vector notation:\n",
    "$$ f_{\\mathbf{w},b}(\\mathbf{x}) = \\mathbf{w} \\cdot \\mathbf{x} + b  \\tag{2} $$ \n",
    "where $\\cdot$ is a vector `dot product`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c9d0ebb-7ed9-482b-b834-d96ffb94d108",
   "metadata": {},
   "source": [
    "### 3.1 single Prediction, vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e162b704-2fb6-48d0-8cea-058930cd3c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x, w, b):\n",
    "    \"\"\"\n",
    "    Single prediction using linear regression\n",
    "\n",
    "    Args:\n",
    "        x (pd.Series): Shape (n,) example with multiple features\n",
    "        w (pd.Series or np.ndarray): Shape (n,) model parameters (weights)\n",
    "        b (scalar): model parameter (bias)\n",
    "\n",
    "    Returns:\n",
    "        p (scalar): prediction\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dot product of x and w (element-wise multiplication and summing)\n",
    "    p = np.dot(x, w) + b\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "690d316f-ccf6-4a79-91d5-862a42dd9be5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions for all samples: [331.85906231 318.54890823 310.44287473 317.74486993 308.54573589\n",
      " 328.70070102 318.97736847 306.24410526 300.54654499 322.09731638\n",
      " 324.50488938 329.27953371 331.10567009 312.9879796  314.80487352\n",
      " 318.08352232 322.36167133 324.53586714 325.48090892 311.56914805\n",
      " 320.38524091 333.63930332 340.68227733 346.89597171 348.24243158\n",
      " 353.06946705 335.34100507 310.47535568 306.67012378 321.16366348\n",
      " 315.16671134 340.52311999 354.59017726 356.56520093 349.98257152\n",
      " 341.80171797 321.99059371 320.11641808 324.9295619  330.70587169\n",
      " 333.0896711  337.61643615 336.40208447 357.67336453 353.24449975\n",
      " 350.12810901 357.4228436  367.21437663 350.16742024 355.51634645\n",
      " 340.83016945 339.86270484 364.77678405 356.20468335 353.81305504\n",
      " 350.63986534 347.21067986 333.20099023 334.67025142 345.41878668\n",
      " 344.3215719  344.72739146 343.57823553 354.29970354 363.82417383\n",
      " 365.55633307 367.59973558 357.89295219 361.49889614 372.86116357\n",
      " 378.81584291 381.01322177 369.41798448 362.52368768 361.41708814\n",
      " 375.27248891 374.43395042 349.4620088  345.20193484 342.09961844\n",
      " 362.5568949  393.22054101 375.23018023 378.42460198 393.78996205\n",
      " 373.01257732 370.97388387 372.36956804 372.11389958 375.58098284\n",
      " 375.95610741 359.31334008 358.45237836 360.970969   363.04717579\n",
      " 364.97139593 367.65882937 396.00003034 398.45945099 389.01889753\n",
      " 386.60286529 377.19064629 380.90780229 383.98390754 394.04877333\n",
      " 387.04213123 399.17017085 408.26238875 404.55685463 379.1011266\n",
      " 380.43508989 395.17756209 378.43741188 394.7885436  387.08315682\n",
      " 389.00135847 378.2948647  370.91716493 375.15651064 403.85757617\n",
      " 416.66045453 416.64334596 390.92920948 391.80027123 386.01357694\n",
      " 383.91363117 408.05288047 403.85362248 411.22928376 421.74076026\n",
      " 425.39270993 394.81893873 399.00391465 413.63382599 422.86791659\n",
      " 406.49878361 403.98590967 404.4349172  419.75228734 410.9791096\n",
      " 421.55910537 427.5215512  427.42074288 436.9885848  421.36753442\n",
      " 417.32168856 411.74201349 425.10380772 438.08906179 411.98478878\n",
      " 435.06242506 436.08576149 426.83151732 425.16739448 428.18116477\n",
      " 417.79163491 418.85903344 414.11386563 412.98765929 403.87494876\n",
      " 419.77656073 406.09236595 427.52644386 426.60243476 440.4180601\n",
      " 435.29939772 416.6670476  424.44713705 408.02965435 423.26657314\n",
      " 425.90415078 451.42867747 440.40624088 442.61357533 440.89517462\n",
      " 440.88883718 451.54831392 439.97406519 431.12267253 428.84800818\n",
      " 424.83681102 429.30188131 423.76891186 440.71690461 440.48006855\n",
      " 454.64768479 443.9997768  463.95796333 460.60151533 455.69172341\n",
      " 455.44877067 455.12892537 456.09706209 469.65896837 449.18835193\n",
      " 441.96067358 440.50550426 445.00907898 447.72289195 450.66737211\n",
      " 452.22877504 453.57289725 479.7605626  475.58862952 441.19237407\n",
      " 436.19504057 453.48820561 452.22697478 448.75285591 447.09623886\n",
      " 469.28442865 473.28336942 485.3761232  482.0306918  480.00849687\n",
      " 480.09788296 472.59179854 471.11731906 473.67823554 462.55117643\n",
      " 464.35235892 469.13232932 478.11057974 463.20565642 459.53393146\n",
      " 451.1990096  464.34514227 469.46459027 476.43822151 481.64935276\n",
      " 471.96518529 477.2053982  471.71023375 463.98280106 492.61272738\n",
      " 489.24831188 488.96635976 494.31679157 473.64648855 462.68991897\n",
      " 462.23847539 480.35231314 491.96206227 493.25688891 482.15771283\n",
      " 496.38967517 485.12123216 480.8314199  495.23107533 493.83663218\n",
      " 490.7798457  486.16644003 489.17697313 510.38303988 499.08279885\n",
      " 487.10680151 485.71470188 499.41065133 503.29495951 512.85515067\n",
      " 507.13511471 492.70586049 489.38584731 504.65241089 506.18294225\n",
      " 494.75430601 494.80965912 499.32015323 513.88701274 497.82525932\n",
      " 493.66462923 484.79102659 480.60923343 496.94502737 500.97267121\n",
      " 512.58247444 522.09265409 508.42877782 500.62846261 498.12900735\n",
      " 505.99500114 513.81918206 508.92259444 517.18858107 535.9634173\n",
      " 530.15003474 535.7024628  526.17395795 515.5097874  515.31052648\n",
      " 508.28361097 501.62940413 502.79921988 513.08042001 517.27038866\n",
      " 518.71462542 516.96957101 530.20520585 537.81496542 516.27741813\n",
      " 517.2116676  526.85866326 529.64873775 532.63150064 523.17180349\n",
      " 533.11225057 536.08299345 539.5133661  527.20174642 525.70277669\n",
      " 534.13591334 543.95072277 530.99342078 519.5834879  524.67296083\n",
      " 526.78218542 517.62863117 518.92254722 544.1939977  549.20820248\n",
      " 540.01029361 545.10556942 537.87753933 529.07356518 539.33448415\n",
      " 553.70574304 526.00147273 523.70250998 553.48293779 525.09205771\n",
      " 556.69465232 541.15735603 540.66421223 550.89601968 547.23954224\n",
      " 559.4783291  552.95483938 569.76150865 559.24755635 560.15257372\n",
      " 549.82434555 563.12275688 547.38856142 544.34591439 533.69045268\n",
      " 551.35077383 542.65240939 537.37873008 542.89048742 554.58335931\n",
      " 561.90169104 570.02745286 548.93994819 547.81481089 544.08571654\n",
      " 563.05024735 574.55889161 552.33285137 562.10214854 568.68111491\n",
      " 574.38497757 586.4038478  591.19557202 559.31115446 567.14723079\n",
      " 585.50763944 574.78040387 563.01467307 552.15280479 557.75799152\n",
      " 567.27523423 582.91222398 597.24532613 581.19320968 574.24563247\n",
      " 565.05077272 558.78976017 554.15344655 565.15706795 573.56126901\n",
      " 586.31956401 584.39810957 592.12381459 568.81607787 608.00122132\n",
      " 606.28650991 572.09453073 577.68524435 566.36815663 592.21748982\n",
      " 584.1802487  589.91230059 600.94579034 590.21687404 604.9977033\n",
      " 600.22993643 600.59589827 609.93447146 590.59350922 613.89995559\n",
      " 583.87655177 594.52764253 604.72585532 613.18267678 592.07312834\n",
      " 585.15378662 603.90485655 584.54743951 584.38638651 585.03909269\n",
      " 587.03744301 595.96993885 601.00940626 603.76221836 611.82535059\n",
      " 616.75847047 605.67795552 606.29860079 603.66100445 600.81011719\n",
      " 593.47949944 617.09070649 618.50500685 632.88106702 624.66943413\n",
      " 623.02913507 611.53114613 609.8635908  614.1588341  640.77038756\n",
      " 613.24516577 622.93327133 628.48550755 622.12751854 611.02791866\n",
      " 613.99766723 616.20564914 621.25000837 624.74051357 618.46175663\n",
      " 613.14639498 637.85043914 642.62038935 635.09883428 637.83654718\n",
      " 642.85594005 643.42783876 634.23587359 627.18665646 629.17215476\n",
      " 637.38983331 642.56974932 647.33022972 635.24609145 628.57026966\n",
      " 623.05908538 618.58220848 615.17328606 631.47896765 651.61119213\n",
      " 642.26672728 625.40077909 632.43839502 631.24508809 622.8621407\n",
      " 631.06447973 640.56594711 645.62726496 652.87154356 656.61328838\n",
      " 651.99566306 640.94678761 660.60892579 646.70112546 641.9605893\n",
      " 634.71967988 638.75776624 644.72601495 653.35035435 662.25658581\n",
      " 656.52123409 659.62405492 666.72186125 646.05050001 656.67128822\n",
      " 650.5102607  657.53590532 670.13226188 665.53124326 650.16964545\n",
      " 652.39435111 643.65116586 646.14199545 645.2217822  648.22217519\n",
      " 679.086405   687.29178937 682.92412634 663.0991322  678.9027644 ]\n"
     ]
    }
   ],
   "source": [
    "# Make a prediction\n",
    "f_wb = predict(x_train, w_init, b_init)\n",
    "\n",
    "print(\"Predictions for all samples:\", f_wb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3948f508-1173-4c60-92eb-e4a31f1c3534",
   "metadata": {},
   "source": [
    "### 4.Compute Cost With Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "195ef78b-726a-4d3d-8807-d19d83599463",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(X, y, w, b): \n",
    "    \"\"\"\n",
    "    Compute cost using pandas for linear regression.\n",
    "    \n",
    "    Args:\n",
    "      X (pd.DataFrame): Data, m examples with n features\n",
    "      y (pd.Series)   : Target values\n",
    "      w (pd.Series)   : Model parameters (weights)\n",
    "      b (scalar)      : Model parameter (bias)\n",
    "      \n",
    "    Returns:\n",
    "      cost (scalar): The computed cost\n",
    "    \"\"\"\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Compute predictions: f_wb = X.dot(w) + b\n",
    "    predictions = X.dot(w) + b  # Shape: (m,)\n",
    "    \n",
    "    # Compute the cost: J(w, b) = (1/2m) * sum((f_wb_i - y_i)^2)\n",
    "    cost = ((predictions - y) ** 2).sum() / (2 * m)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a4ef89a2-292f-4dfe-b6a6-8ca9ffe38854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost at optimal w : 124488.76611348269\n"
     ]
    }
   ],
   "source": [
    "cost = compute_cost(x_train, y_train, w_init, b_init)\n",
    "print(f'Cost at optimal w : {cost}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5a3648-cf0e-4d83-816a-5afa3be8cdc3",
   "metadata": {},
   "source": [
    "### 5. Gradient Descent With Multiple Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75673309-5dd9-4034-9447-b150be799dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression using pandas.\n",
    "\n",
    "    Args:\n",
    "      X (pd.DataFrame): Data, m examples with n features\n",
    "      y (pd.Series): target values\n",
    "      w (np.ndarray): model parameters (weights)\n",
    "      b (scalar): model parameter (bias)\n",
    "\n",
    "    Returns:\n",
    "      dj_dw (np.ndarray): The gradient of the cost w.r.t. the parameters w. \n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape  # number of examples (m) and number of features (n)\n",
    "    dj_dw = np.zeros(n)  # Initialize the gradient for weights\n",
    "    dj_db = 0  # Initialize the gradient for bias\n",
    "\n",
    "    # Loop through all examples\n",
    "    for i in range(m):\n",
    "        f_wb = np.dot(X.iloc[i], w) + b  # Model prediction\n",
    "        error = f_wb - y.iloc[i]         # Error: prediction - actual value\n",
    "        \n",
    "        # Update gradients\n",
    "        dj_db += error\n",
    "        dj_dw += error * X.iloc[i].values  # element-wise product of error and feature\n",
    "        \n",
    "    dj_dw /= m  # Average gradient for weights\n",
    "    dj_db /= m  # Average gradient for bias\n",
    "\n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ef822654-1b55-4810-acde-1bb1444289c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to learn w and b.\n",
    "\n",
    "    Args:\n",
    "      X (pd.DataFrame or np.ndarray): Data, m examples with n features\n",
    "      y (pd.Series or np.ndarray): target values\n",
    "      w_in (np.ndarray): initial model parameters  \n",
    "      b_in (scalar): initial model parameter\n",
    "      cost_function: function to compute cost\n",
    "      gradient_function: function to compute the gradient\n",
    "      alpha (float): Learning rate\n",
    "      num_iters (int): number of iterations to run gradient descent\n",
    "\n",
    "    Returns:\n",
    "      w (np.ndarray): Updated weights \n",
    "      b (scalar): Updated bias\n",
    "      J_history (list): Cost history over iterations\n",
    "    \"\"\"\n",
    "    \n",
    "    J_history = []  # List to store cost at each iteration\n",
    "    w = np.copy(w_in)  # Avoid modifying original w\n",
    "    b = b_in\n",
    "    \n",
    "    # Gradient descent loop\n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Compute gradients\n",
    "        dj_db, dj_dw = gradient_function(X, y, w, b)\n",
    "\n",
    "        # Gradient clipping (optional)\n",
    "        max_gradient = 1e4  # Threshold to clip the gradients (aggressive clipping)\n",
    "        dj_dw = np.clip(dj_dw, -max_gradient, max_gradient)\n",
    "        dj_db = np.clip(dj_db, -max_gradient, max_gradient)\n",
    "\n",
    "        # Update parameters (weights and bias)\n",
    "        w -= alpha * dj_dw\n",
    "        b -= alpha * dj_db\n",
    "\n",
    "        # Compute and store the cost at each iteration\n",
    "        J_history.append(cost_function(X, y, w, b))\n",
    "\n",
    "        # Print the cost every 10 iterations\n",
    "        if i % max(1, num_iters // 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]:8.2f}\")\n",
    "\n",
    "    return w, b, J_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "e4d6fa7a-adf5-4363-90a5-9c30334b7c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost     2.38\n",
      "Iteration  100: Cost     0.01\n",
      "Iteration  200: Cost     0.01\n",
      "Iteration  300: Cost     0.01\n",
      "Iteration  400: Cost     0.01\n",
      "Iteration  500: Cost     0.01\n",
      "Iteration  600: Cost     0.01\n",
      "Iteration  700: Cost     0.01\n",
      "Iteration  800: Cost     0.01\n",
      "Iteration  900: Cost     0.01\n",
      "Final weights: [ 2.65240622e-05 -3.98296060e-04  7.01771478e-03  1.24036394e-03\n",
      "  9.75474123e-03  5.36392819e-03  3.87731375e-03  5.09973415e-03]\n",
      "Final bias: -3.615700555180322e-05\n"
     ]
    }
   ],
   "source": [
    "#learning rate, iteration\n",
    "alpha =  1e-5\n",
    "num_iters = 1000\n",
    "\n",
    "w_final, b_final, cost_history = gradient_descent(x_train, y_train, w_init, b_init, compute_cost, compute_gradient, alpha, num_iters)\n",
    "\n",
    "print(\"Final weights:\", w_final)\n",
    "print(\"Final bias:\", b_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
